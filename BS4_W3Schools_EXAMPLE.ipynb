{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS6F894xysHSboZmsvzSqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CIS9650/blob/main/BS4_W3Schools_EXAMPLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beautiful Soup (BS4) - W3Schools Example  \n",
        "  \n",
        "*by Professor Patrick, 2024.*  "
      ],
      "metadata": {
        "id": "hxxzW8wHHu9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `w3schools.com` Example - Introduction  \n",
        "\n",
        "This notebook demonstrates how to use **Beautiful Soup** (BS4) to scrape data from the **W3Schools** HTML tutorial page. We'll fetch the HTML content, parse it, and extract specific elements like the title, headings, links, and specific sections.  "
      ],
      "metadata": {
        "id": "x62-T9gxJCSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Housekeeping: Importing Libraries  \n",
        "  "
      ],
      "metadata": {
        "id": "RCVIfriYI8r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Required Libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "NCdTMlYmI6Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching the Webpage  \n",
        "\n",
        "We send a `GET` request to the URL and check the HTTP response code to ensure the page was fetched successfully."
      ],
      "metadata": {
        "id": "mWwMq57-JUdO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JasP-y7HlRD"
      },
      "outputs": [],
      "source": [
        "# Step 1: Sending a Request to the URL\n",
        "url = \"https://www.w3schools.com/html/\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Checking the Response\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully fetched the webpage!\")\n",
        "else:\n",
        "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing and Exploring the Page Title  \n",
        "  \n",
        "We use Beautiful Soup to parse the HTML and extract the title of the page.  "
      ],
      "metadata": {
        "id": "d1So0je2I9Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Parsing the HTML Content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Step 3: Exploring the Page Title\n",
        "page_title = soup.title.string\n",
        "print(\"Page Title:\", page_title)"
      ],
      "metadata": {
        "id": "11PGDqHvI0ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Headings  \n",
        "  \n",
        "Using the find_all method, we extract all headings (h1, h2, and h3) and print them with their respective tag names."
      ],
      "metadata": {
        "id": "5A7mm5NiI9sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Extracting All Headings\n",
        "headings = soup.find_all(['h1', 'h2', 'h3'])\n",
        "print(f\"Captured {len(headings)} Headings as a {type(headings)} named 'headings'.\")\n",
        "print(\"\\nAll Headings on the Page:\")\n",
        "for heading in headings:\n",
        "    print(f\"{heading.name}: {heading.text.strip()}\")"
      ],
      "metadata": {
        "id": "8VwA7TP5IyCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrangling Results to a Dictionary  \n",
        "  \n",
        "The following Python code creates a `headings_dict` dictionary with `headingTag` and `headingName` lists (arrays) for every element of the `headings` bs4.element.ResultSet:"
      ],
      "metadata": {
        "id": "q-q4p9zjL0LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create a dictionary to store headings\n",
        "headings_dict = {\n",
        "    \"headingTag\": [],\n",
        "    \"headingName\": []\n",
        "}\n",
        "\n",
        "# Populate the dictionary\n",
        "for heading in headings:\n",
        "    headings_dict[\"headingTag\"].append(heading.name)  # Add the heading tag type (e.g., h1, h2)\n",
        "    headings_dict[\"headingName\"].append(heading.text.strip())  # Add the text of the heading\n",
        "\n",
        "# Display the dictionary\n",
        "print(\"Headings Dictionary:\")\n",
        "print(headings_dict)\n"
      ],
      "metadata": {
        "id": "FYWfOtgZMIEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Valid Links  \n",
        "  \n",
        "The following code extracts valid links by:\n",
        "- extracting all anchor tags (`<a>`)\n",
        "- filtering them for valid links using `href`\n",
        "- further filtering out any that invoke JavaScript code\n",
        "- then printing the text and URL for the first 10 valid links.  "
      ],
      "metadata": {
        "id": "W6NCe33YI-Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Extracting Links\n",
        "links = soup.find_all('a', href=True)\n",
        "# Extracting and Filtering Valid Links\n",
        "valid_links = []\n",
        "\n",
        "for link in links:\n",
        "    href = link.get('href', '')\n",
        "    # Check if the href is a valid URL (not empty, not starting with \"javascript:\")\n",
        "    if href and not href.startswith('javascript:'):\n",
        "        valid_links.append({\n",
        "            \"text\": link.text.strip(),\n",
        "            \"url\": href\n",
        "        })\n",
        "\n",
        "print(f\"Captured {len(valid_links)} valid Links as a {type(valid_links)} named 'valid_links'.\")\n",
        "\n",
        "print(\"\\nThe first 10 Links on the Page:\")\n",
        "for valid_link in valid_links[:10]:  # Limiting to the first 10 valid links for readability\n",
        "    print(f\"Text: {valid_link['text']}, URL: {valid_link['url']}\")"
      ],
      "metadata": {
        "id": "wWRDhc4aH1vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting a Specific Section  \n",
        "  \n",
        "We target a specific section of the page using its id attribute (main) and extract its content, limiting the output to 500 characters for readability.  "
      ],
      "metadata": {
        "id": "GDD8S-KdI-sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Extracting a Specific Section\n",
        "tutorial_section = soup.find('div', {'id': 'main'})\n",
        "if tutorial_section:\n",
        "    print(\"\\nStart of Main Tutorial Section Content:\")\n",
        "    print(tutorial_section.text.strip()[:500])  # Displaying the first 500 characters\n",
        "else:\n",
        "    print(\"Main Tutorial Section not found.\")"
      ],
      "metadata": {
        "id": "h56PcYwvHzSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion  \n",
        "   \n",
        "This notebook section provides an overview of scraping content using **Beautiful Soup**, focusing on the title, headings, links, and specific page sections. Modify the code to explore additional elements or extract deeper insights. Always follow ethical scraping practices and comply with the website's terms of service.  "
      ],
      "metadata": {
        "id": "bxLCD2i-KBrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "r_sIwGLEKObm"
      }
    }
  ]
}